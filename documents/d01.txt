Given a character sequence and a defined document unit, 
tokenization is the task of chopping it up into pieces, called tokens, 
perhaps at the same time throwing away certain characters, 
such as punctuation. 

