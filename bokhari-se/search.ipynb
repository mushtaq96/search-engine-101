{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sbokhari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sbokhari/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string, os, nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stop_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sbokhari/AIS-Assignment-2/Assignment-2/search.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sbokhari/AIS-Assignment-2/Assignment-2/search.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# process the stop words\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sbokhari/AIS-Assignment-2/Assignment-2/search.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mstop_words.txt\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sbokhari/AIS-Assignment-2/Assignment-2/search.ipynb#ch0000001vscode-remote?line=2'>3</a>\u001b[0m     doc \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sbokhari/AIS-Assignment-2/Assignment-2/search.ipynb#ch0000001vscode-remote?line=3'>4</a>\u001b[0m     processed_doc \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stop_words.txt'"
     ]
    }
   ],
   "source": [
    "# process the stop words\n",
    "with open('stop_words.txt') as f:\n",
    "    doc = f.readlines()\n",
    "    processed_doc = []\n",
    "    for line in doc:\n",
    "        new_line = line.strip()\n",
    "        new_line = new_line.translate(str.maketrans('', '', string.punctuation)).lower();\n",
    "        processed_doc.append(new_line)\n",
    "    stop_words = []\n",
    "    for line in processed_doc:\n",
    "        stop_words.extend(line.split())\n",
    "    print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents \n",
      " 8 ['Given a character sequence and a defined document unit, \\ntokenization is the task of chopping it up into pieces, called tokens, \\nperhaps at the same time throwing away certain characters, \\nsuch as punctuation. \\n\\n', 's is a spam page.\\ntokens\\nstopwords\\nindex\\npostings\\nclassification\\nsupervised\\ntokens\\nstopwords\\nindex\\npostings\\nclassification\\nsupervised\\ntokens\\nstopwords\\nindex\\npostings\\nclassification\\nsupervised\\ntokens\\nstopwords\\nindex\\npostings\\nclassification\\nsupervised\\n', 'In text classification, we are given a description \\nof a document and a fixed set of classes.\\n\\n', 'Using a supervised learning method or learning algorithm, \\nwe wish to learn a classifier or classification function \\nthat maps documents to classes.\\n\\n', 'For English, an alternative to making every token lowercase \\nis to just make some tokens lowercase. The simplest heuristic\\nis to convert to lowercase words at the beginning of a \\nsentence and all words occurring in a title that is all\\nuppercase or in which most or all words are capitalized.\\n', 'Index the documents that each term occurs in by \\ncreating an inverted index, consisting of a \\ndictionary and postings.\\n', 'Token normalization is the process of canonicalizing <br>\\ntokens so that matches occur despite superficial <br>\\ndifferences in the character sequences of the tokens. <br>\\n\\n', 'To gain the speed benefits of indexing at retrieval time, \\nwe have to build the index in advance. The major steps \\nin this are: Collect the documents to be indexed, \\ntokenize the text, turning each document into a \\nlist of tokens, do linguistic preprocessing, \\nproducing a list of normalized tokens, \\nwhich are the indexing term.\\n']\n"
     ]
    }
   ],
   "source": [
    "# collect the text from the files\n",
    "cwd = os.getcwd()\n",
    "files = os.listdir(\"./documents\")\n",
    "dataset = []\n",
    "for file in files:\n",
    "    path = f\"{cwd}/documents/{file}\"\n",
    "    with open(path) as f:\n",
    "        dataset.append(f.read())\n",
    "print('Number of documents \\n', len(dataset), dataset) # has all the files text in a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['given charact sequenc defin document unit token task chop into piec call token perhap same time throw away certain charact such punctuat', 'spam page token stopword index post classif supervis token stopword index post classif supervis token stopword index post classif supervis token stopword index post classif supervis', 'text classif given descript document fix set class', 'use supervis learn method learn algorithm wish learn classifi classif function map document class', 'english altern make everi token lowercas just make some token lowercas simplest heurist convert lowercas word begin sentenc all word occur titl all uppercas which most all word capit', 'index document each term occur creat invert index consist dictionari post', 'token normal process canonic token match occur despit superfici differ charact sequenc token', 'gain speed benefit index retriev time build index advanc major step collect document index token text turn each document into list token linguist preprocess produc list normal token which index term']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(string):\n",
    "    sentences = []\n",
    "    #lemmatization process\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    #stemming - convert words into their stem\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    x = [i.lower() for i in word_tokenize(string) if i.isalpha()]\n",
    "    for word in x: \n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            sentences.append(porter_stemmer.stem(wordnet.lemmatize(word)))\n",
    "            # sentences.append(word)\n",
    "\n",
    "    new_string = \" \".join(sentences)\n",
    "    # print(new_string)\n",
    "    return new_string\n",
    "\n",
    "#preprossing the data\n",
    "clean_dataset = []\n",
    "for each_document in dataset:\n",
    "    clean_doc = preprocess(each_document)\n",
    "    clean_dataset.append(clean_doc)\n",
    "print(clean_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Document    perhap     token   certain      time      unit     given  \\\n",
      "0       TF  0.045455  0.090909  0.045455  0.045455  0.045455  0.045455   \n",
      "\n",
      "       such      into   sequenc  ...  punctuat      task      same      chop  \\\n",
      "0  0.045455  0.045455  0.045455  ...  0.045455  0.045455  0.045455  0.045455   \n",
      "\n",
      "      throw   charact  document      piec      away      call  \n",
      "0  0.045455  0.090909  0.045455  0.045455  0.045455  0.045455  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "  Document  supervis      spam     token   classif  stopword      page  \\\n",
      "0       TF  0.153846  0.038462  0.153846  0.153846  0.153846  0.038462   \n",
      "\n",
      "       post     index  \n",
      "0  0.153846  0.153846  \n",
      "  Document  given  document    fix    set  class   text  classif  descript\n",
      "0       TF  0.125     0.125  0.125  0.125  0.125  0.125    0.125     0.125\n",
      "  Document  supervis      wish  document     class  classifi    method  \\\n",
      "0       TF  0.071429  0.071429  0.071429  0.071429  0.071429  0.071429   \n",
      "\n",
      "      learn   classif  algorithm  function       use       map  \n",
      "0  0.214286  0.071429   0.071429  0.071429  0.071429  0.071429  \n",
      "  Document     occur     token      word     which      most   sentenc  \\\n",
      "0       TF  0.034483  0.068966  0.103448  0.034483  0.034483  0.034483   \n",
      "\n",
      "   lowercas     begin   convert  ...  simplest     capit  uppercas   english  \\\n",
      "0  0.103448  0.034483  0.034483  ...  0.034483  0.034483  0.034483  0.034483   \n",
      "\n",
      "       some      just      titl      make       all    altern  \n",
      "0  0.034483  0.034483  0.034483  0.068966  0.103448  0.034483  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "  Document  dictionari     occur  document    invert   consist     creat  \\\n",
      "0       TF    0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
      "\n",
      "       each      post      term     index  \n",
      "0  0.090909  0.090909  0.090909  0.181818  \n",
      "  Document   charact     occur  superfici    differ   sequenc    despit  \\\n",
      "0       TF  0.076923  0.076923   0.076923  0.076923  0.076923  0.076923   \n",
      "\n",
      "      token    normal   canonic   process     match  \n",
      "0  0.230769  0.076923  0.076923  0.076923  0.076923  \n",
      "  Document   retriev  preprocess      gain     token   benefit      turn  \\\n",
      "0       TF  0.032258    0.032258  0.032258  0.096774  0.032258  0.032258   \n",
      "\n",
      "       term     index      time  ...      text      list      step     major  \\\n",
      "0  0.032258  0.129032  0.032258  ...  0.032258  0.064516  0.032258  0.032258   \n",
      "\n",
      "      build  document    normal    produc      each   collect  \n",
      "0  0.032258  0.064516  0.032258  0.032258  0.032258  0.032258  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# standard formula to find normalized term frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def calculate_tf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0) #create a new dictinary \n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt) \n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"TF\"]    \n",
    "        df.insert(loc = idx, column = 'Document', value = new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "\n",
    "term_freq = calculate_tf(clean_dataset) # store result for further access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Unique words = 87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'given': 2.386294361119891,\n",
       " 'charact': 2.386294361119891,\n",
       " 'sequenc': 2.386294361119891,\n",
       " 'defin': 3.0794415416798357,\n",
       " 'document': 1.4700036292457357,\n",
       " 'unit': 3.0794415416798357,\n",
       " 'token': 1.4700036292457357,\n",
       " 'task': 3.0794415416798357,\n",
       " 'chop': 3.0794415416798357,\n",
       " 'into': 2.386294361119891,\n",
       " 'piec': 3.0794415416798357,\n",
       " 'call': 3.0794415416798357,\n",
       " 'perhap': 3.0794415416798357,\n",
       " 'same': 3.0794415416798357,\n",
       " 'time': 2.386294361119891,\n",
       " 'throw': 3.0794415416798357,\n",
       " 'away': 3.0794415416798357,\n",
       " 'certain': 3.0794415416798357,\n",
       " 'such': 3.0794415416798357,\n",
       " 'punctuat': 3.0794415416798357,\n",
       " 'spam': 3.0794415416798357,\n",
       " 'page': 3.0794415416798357,\n",
       " 'stopword': 3.0794415416798357,\n",
       " 'index': 1.9808292530117262,\n",
       " 'post': 2.386294361119891,\n",
       " 'classif': 1.9808292530117262,\n",
       " 'supervis': 2.386294361119891,\n",
       " 'text': 2.386294361119891,\n",
       " 'descript': 3.0794415416798357,\n",
       " 'fix': 3.0794415416798357,\n",
       " 'set': 3.0794415416798357,\n",
       " 'class': 2.386294361119891,\n",
       " 'use': 3.0794415416798357,\n",
       " 'learn': 3.0794415416798357,\n",
       " 'method': 3.0794415416798357,\n",
       " 'algorithm': 3.0794415416798357,\n",
       " 'wish': 3.0794415416798357,\n",
       " 'classifi': 3.0794415416798357,\n",
       " 'function': 3.0794415416798357,\n",
       " 'map': 3.0794415416798357,\n",
       " 'english': 3.0794415416798357,\n",
       " 'altern': 3.0794415416798357,\n",
       " 'make': 3.0794415416798357,\n",
       " 'everi': 3.0794415416798357,\n",
       " 'lowercas': 3.0794415416798357,\n",
       " 'just': 3.0794415416798357,\n",
       " 'some': 3.0794415416798357,\n",
       " 'simplest': 3.0794415416798357,\n",
       " 'heurist': 3.0794415416798357,\n",
       " 'convert': 3.0794415416798357,\n",
       " 'word': 3.0794415416798357,\n",
       " 'begin': 3.0794415416798357,\n",
       " 'sentenc': 3.0794415416798357,\n",
       " 'all': 3.0794415416798357,\n",
       " 'occur': 1.9808292530117262,\n",
       " 'titl': 3.0794415416798357,\n",
       " 'uppercas': 3.0794415416798357,\n",
       " 'which': 2.386294361119891,\n",
       " 'most': 3.0794415416798357,\n",
       " 'capit': 3.0794415416798357,\n",
       " 'each': 2.386294361119891,\n",
       " 'term': 2.386294361119891,\n",
       " 'creat': 3.0794415416798357,\n",
       " 'invert': 3.0794415416798357,\n",
       " 'consist': 3.0794415416798357,\n",
       " 'dictionari': 3.0794415416798357,\n",
       " 'normal': 2.386294361119891,\n",
       " 'process': 3.0794415416798357,\n",
       " 'canonic': 3.0794415416798357,\n",
       " 'match': 3.0794415416798357,\n",
       " 'despit': 3.0794415416798357,\n",
       " 'superfici': 3.0794415416798357,\n",
       " 'differ': 3.0794415416798357,\n",
       " 'gain': 3.0794415416798357,\n",
       " 'speed': 3.0794415416798357,\n",
       " 'benefit': 3.0794415416798357,\n",
       " 'retriev': 3.0794415416798357,\n",
       " 'build': 3.0794415416798357,\n",
       " 'advanc': 3.0794415416798357,\n",
       " 'major': 3.0794415416798357,\n",
       " 'step': 3.0794415416798357,\n",
       " 'collect': 3.0794415416798357,\n",
       " 'turn': 3.0794415416798357,\n",
       " 'list': 3.0794415416798357,\n",
       " 'linguist': 3.0794415416798357,\n",
       " 'preprocess': 3.0794415416798357,\n",
       " 'produc': 3.0794415416798357}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how important a word is to a document in a collection of files\n",
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    for doc in range (0, len(allDocuments)):\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def compute_idf(documents):\n",
    "    idf_dict = {}\n",
    "    for doc in documents:\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
    "    return idf_dict\n",
    "idf_dict = compute_idf(clean_dataset)\n",
    "print(\"Total number of Unique words =\", len(idf_dict))\n",
    "compute_idf(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into a text file\n",
    "with open('index.txt', 'w') as f:\n",
    "    for key, value in idf_dict.items():\n",
    "        f.write('%s:%s\\n' % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf score across all docs for the query string\n",
    "def calculate_tfidf_with_alldocs(documents , query):\n",
    "    tf_idf = []\n",
    "    index = 0\n",
    "    query_tokens = query.split()\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = term_freq[index]\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            for text in query_tokens:\n",
    "                if(text == word):\n",
    "                    # idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1\n",
    "    df.fillna(0 , axis = 1, inplace = True)\n",
    "    return tf_idf , df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc  tokens\n",
      "0    0       0\n",
      "1    1       0\n",
      "2    2       0\n",
      "3    3       0\n",
      "4    4       0\n",
      "5    5       0\n",
      "6    6       0\n",
      "7    7       0\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"tokens\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc     token\n",
      "0  0.0  0.133637\n",
      "1  1.0  0.226154\n",
      "2  2.0  0.000000\n",
      "3  3.0  0.000000\n",
      "4  4.0  0.101380\n",
      "5  5.0  0.000000\n",
      "6  6.0  0.339232\n",
      "7  7.0  0.142258\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"token\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc     index\n",
      "0  0.0  0.000000\n",
      "1  1.0  0.304743\n",
      "2  2.0  0.000000\n",
      "3  3.0  0.000000\n",
      "4  4.0  0.000000\n",
      "5  5.0  0.360151\n",
      "6  6.0  0.000000\n",
      "7  7.0  0.255591\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"index\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc  classification\n",
      "0    0               0\n",
      "1    1               0\n",
      "2    2               0\n",
      "3    3               0\n",
      "4    4               0\n",
      "5    5               0\n",
      "6    6               0\n",
      "7    7               0\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"classification\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc   classif\n",
      "0  0.0  0.000000\n",
      "1  1.0  0.304743\n",
      "2  2.0  0.247604\n",
      "3  3.0  0.141488\n",
      "4  4.0  0.000000\n",
      "5  5.0  0.000000\n",
      "6  6.0  0.000000\n",
      "7  7.0  0.000000\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"classif\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc     token   classif\n",
      "0  0.0  0.133637  0.000000\n",
      "1  1.0  0.226154  0.304743\n",
      "2  2.0  0.000000  0.247604\n",
      "3  3.0  0.000000  0.141488\n",
      "4  4.0  0.101380  0.000000\n",
      "5  5.0  0.000000  0.000000\n",
      "6  6.0  0.339232  0.000000\n",
      "7  7.0  0.142258  0.000000\n"
     ]
    }
   ],
   "source": [
    "tf_idf , df = calculate_tfidf_with_alldocs(clean_dataset , \"token classif\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
